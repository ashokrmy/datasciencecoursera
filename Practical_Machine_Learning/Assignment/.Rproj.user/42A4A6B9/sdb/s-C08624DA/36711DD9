{
    "contents" : "---\ntitle: \"Predicting Human Activity\"\nauthor: \"Diego López Pozueta\"\ndate: \"July 13th, 2014\"\noutput: html_document\n---\n\n# Executive Summary\n\nUsing devices such as [*Jawbone Up*](https://jawbone.com/up), [*Nike FuelBand*](http://www.nike.com/us/en_us/c/nikeplus-fuelband) or [*Fitbit*](http://www.fitbit.com/) it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement – a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. \n\nThe goal of this project is to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants where they were asked to perform barbell lifts correctly and incorrectly in 5 different ways; and build a model to predict the manner in which they did the exercise. \n\n# Dataset \n\nThe dataset for this project is the [*Weight Lifting Exercises Dataset*](http://groupware.les.inf.puc-rio.br/har). \n\nHuman activity recognition research has traditionally focused on discriminating between different activities, i.e. to predict \"which\" activity was performed at a specific point in time. The approach proposed for the Weight Lifting Exercises dataset is to investigate \"how (well)\" an activity was performed. \n\nSix young health participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E). Class A corresponds to the specified execution of the exercise, while the other 4 classes correspond to common mistakes. \n\nFor more information please visit http://groupware.les.inf.puc-rio.br/har.\n\nFor this project, we have a training and a testing dataset and both can be downloaded here:\n\n* training dataset: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv\n* testing dataset: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv\n\n## Download dataset \n\n```{r warning=FALSE, message=FALSE}\n# dataset folder to store the training and testing dataset\ndata.folder <- \"./Datasets/\"\n# current date in format YYYYMMDD\ndate <- gsub(pattern=\"-\", replacement = \"\", Sys.Date()) \n# download training dataset  \ntraining.url <- \"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv\"\ntraining.file.name <- paste(data.folder,\"training_\",date,\".csv\", sep=\"\")\ndownload.file(training.url, training.file.name , method = \"curl\")\n# download testing dataset  \ntesting.url <- \"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv\"\ntesting.file.name <- paste(data.folder,\"testing_\",date,\".csv\", sep=\"\")\ndownload.file(testing.url, testing.file.name , method = \"curl\")\n```\n\n## Read dataset\n\n```{r warning=FALSE, message=FALSE}\n# read training and testing datasets\ntraining <- read.csv(training.file.name, header = TRUE, stringsAsFactors = FALSE)\ntesting <- read.csv(testing.file.name, header = TRUE, stringsAsFactors = FALSE)\n# display structure of training and testing datasets\nstr(training) # 160 variables, 19622 observations\nstr(testing) # 160 variables, 20 observations\n```\n\nThe variable to predict is *classe* in the training set. This variable does not appear in the testing set as this is the value to predict for the 20 test observations.\n\n## Preprocess dataset\n\nThe first 7 columns are identifiers and can be dropped.\n```{r warning=FALSE, message=FALSE}\n# get the name of the identifiers\n(identifiers <- names(training)[1:7])\nvars.identifiers <- names(training) %in% identifiers \n# exclude identifiers\ntraining2 <- training[,!vars.identifiers]\ntesting2 <- testing[,!vars.identifiers]\n```\n\nSome variables were imported as factors or logical values but we know that they are all numeric. Let's convert them.\n```{r warning=FALSE, message=FALSE}\n# convert to character then to numeric\n# char \"\" is converted to NAs\ntraining3 <- as.data.frame(apply(training2, 2,  function(x)as.numeric(as.character(x))))\ntesting3 <- as.data.frame(apply(testing2, 2,  function(x)as.numeric(as.character(x))))\n# bring back the classe variable \ntraining3$classe <- training$classe\n```\n\nIt seems like a lot of variables have missing values\n```{r warning=FALSE, message=FALSE}\n# number of missing values by column\nna.values.cols <- sapply(training3, function(x)sum(is.na(x)))\ntable(na.values.cols) # 53 variables without missing values\nsum(na.values.cols>0) # 100 variables have at least 19216 NA's\n```\n\nThere are 100 variables with more than 98% of missing values. For simplicity we drop them.\n```{r warning=FALSE, message=FALSE}\n# get the name of the variables to exclude\nna.values.cols.names <- names(na.values.cols [na.values.cols > 0])\nvars.nas <- names(training3) %in% na.values.cols.names \n# exclude NAs variables\ntraining4 <- training3[,!vars.nas]\ntesting4 <- testing3[,!vars.nas]\n```\n\nWe have reduced the original dataset containing 160 features to only 53 by dropping all the features having more than 98% of missing values.\n\nFinally let's convert the class variable *classe* to a factor variable.\n```{r warning=FALSE, message=FALSE}\ntraining4$classe <- as.factor(training4$classe)\n```\n\nIt's important to check that the training and testing datasets contain the same variables and they have the same type, so when we apply the predictive model built with the training set to the testing set we don't get any errors.\n```{r warning=FALSE, message=FALSE}\n# check training and testing have same variables\nnames(training4[,-53]) == names(testing4[-53])\n# check training and testing variables have the same type\nsapply(training4[,-53],class) == sapply(testing4[-53],class)\n```\n\n## Explore dataset\n\nSince we still have 53 variables and due to the nature of the dataset, this is, data collected from accelerometers on the belt, forearm, arm and dumbell; chances are that some variables are highly correlated. \n\nLet's plot heatmap of the correlation between variables.\n```{r warning=FALSE, message=FALSE, fig.width = 10, fig.height=10}\n# correlation matrix between features\ncorr.matrix <- cor(training4[,-53])\n# plot heatmap of correlation\nlibrary(ggplot2)\nlibrary(reshape2)\nqplot(x=Var1, y=Var2, data=melt(corr.matrix), fill=value, geom=\"tile\") +\n  scale_fill_gradient2(limits=c(-1, 1)) + \n  theme(axis.text.x = element_text(angle = 90, hjust = 1))\n```\n\nExcluding the diagonal, as the correlation between a variable and itself is always 1, we can see that some features are highly positive correlated (dark blue), or highly negative correlated (dark red).\n\nUsing the ***findCorrelation()*** function in ***caret*** package we can find the highly correlated variables that can be dropped. Dropping highly correlated variables reduces the complexity without loosing precision.\n```{r warning=FALSE, message=FALSE}\nlibrary(caret)\nhigh.cor.var <- findCorrelation(corr.matrix, cutoff = .80, verbose = FALSE)\n# highly correlated variables\n(high.cor.var.names <- rownames(corr.matrix)[high.cor.var])\n```\n\nExamining some examples of variables to drop.\n```{r warning=FALSE, message=FALSE}\n# Example highly negative correlation accel_belt_z\nsubset(corr.matrix[\"accel_belt_z\", ], (abs(corr.matrix[\"accel_belt_z\", ])>0.8) == TRUE)\n# Example highly positive correlation accel_belt_z\nsubset(corr.matrix[\"gyros_forearm_z\", ], (abs(corr.matrix[\"gyros_forearm_z\", ])>0.8) == TRUE)\n```\n\nFinally exclude the highly correlated variables.\n```{r warning=FALSE, message=FALSE}\ntraining5 <- training4[,-high.cor.var]\ntesting5 <- testing4[,-high.cor.var]\n```\n\nThe final training set used has 40 features used to predict the class variable.\n\n# Predictive Model\n\nIt's time to build the predictive model. Here I used and compared a CART and a [random forest](http://download.springer.com/static/pdf/639/art%253A10.1023%252FA%253A1010933404324.pdf?auth66=1405538457_0f5fd1af13b0c3ec4a5b9927b642e3e6&ext=.pdf) model. Seems reasonable to use a classification tree as it has been proven to be an efficient algorithm for highly dimensional datasets, in this particular case we ended with 40 features. Random Forest enhances a single decision tree, by growing multiple decision trees. In general, random forest have high accuracy and do not overfit.\n\n## Validation set\n\nSplit the training set into 2 subsets for training (60%) and validation (40%).\n```{r warning=FALSE, message=FALSE}\nlibrary(caret)\n# set seed for reproducibility\nset.seed(12345) \n# randomly select 60% for training and 40% for validation\ninTrain <- createDataPartition(training5$classe, p = 0.6, list = FALSE)\ntrain.set <- training5[ inTrain,]\nvalidation.set <- training5[-inTrain,]\n```\n\n\n## Build Model\n\nFirst, a single CART model is built.\n```{r warning=FALSE, message=FALSE}\nlibrary(rpart)\n# set seed for reproducibility\nset.seed(12345) \n# train CART model\nmodel.rpart <- rpart(classe~., data = train.set)\n# display the results \nprintcp(model.rpart) \n# visualize cross-validation results \n# plotcp(model.rpart) \n# detailed summary of splits\n# summary(model.rpart) \n# see how well performs on the validation set\npredict.model.rpart <- predict(model.rpart, validation.set, type=\"class\")\nconfusionMatrix(predict.model.rpart, validation.set$classe)\n```\nWe can see that the CART model actually uses only 17 variables to build the decision tree, and has a 71.37% accuracy on the validation set.\n\nLet's try now a random forest.\n```{r warning=FALSE, message=FALSE, fig.width = 10, fig.height=10}\nlibrary(randomForest)\n# set seed for reproducibility\nset.seed(12345) \n# train random forest model\nmodel.rf <- randomForest(classe~., data=train.set)\n# view results \nprint(model.rf) \n# importance of each predictor\nvarImpPlot(model.rf) \n# check random forest model on validation set\npredict.model.rf <- predict(model.rf, validation.set)\nconfusionMatrix(predict.model.rf, validation.set$classe)\n```\n\nThe random forest has out-of-bag error rate of 0.91%, and a 99.31% accuracy on the validation set. Random Forest 30% better accuracy than the CART model.\n\n## Test Model\n\nLet's use the random forest model to predict the class of the testing set.\n```{r warning=FALSE, message=FALSE}\ntesting5$classe <- predict(model.rf, testing5)\n```\n\nAnd create the separate files containing the answers for the 20 test observations.\n```{r warning=FALSE, message=FALSE}\npml_write_files = function(x){\n    n = length(x)\n    for(i in 1:n){\n      filename = paste0(\"problem_id_\",i,\".txt\")\n      write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)\n    }\n}\npml_write_files(as.character(testing5$classe))\n```\n\n\n# Bibliography\n\n* Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. [*Qualitative Activity Recognition of Weight Lifting Exercises*](http://groupware.les.inf.puc-rio.br/har). Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.\n\n*  Breiman, Leo (2001). [*Random Forests*](http://download.springer.com/static/pdf/639/art%253A10.1023%252FA%253A1010933404324.pdf?auth66=1405538457_0f5fd1af13b0c3ec4a5b9927b642e3e6&ext=.pdf). Machine Learning 45 (1): 5–32. \n\n\n",
    "created" : 1405241608645.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "952438783",
    "id" : "36711DD9",
    "lastKnownWriteTime" : 1405369895,
    "path" : "~/Dropbox/Data Science Specialization/8 - Practical Machine Learning/Assignment/pml_assignment.Rmd",
    "project_path" : "pml_assignment.Rmd",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "source_on_save" : false,
    "type" : "r_markdown"
}